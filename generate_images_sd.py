#!/usr/bin/env python3
"""
Gerador de imagens com Stable Diffusion
Otimizado para NVIDIA RTX 3050 (8GB VRAM)
"""

import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from PIL import Image
import json
import os
import sys
import time

# Configura√ß√µes otimizadas para RTX 3050
CONFIG = {
    "model": "runwayml/stable-diffusion-v1-5",  # Modelo r√°pido e eficiente
    "num_inference_steps": 30,  # 30 = r√°pido, 50 = melhor qualidade
    "guidance_scale": 7.5,
    "width": 800,
    "height": 1200,
    "negative_prompt": "ugly, blurry, low quality, distorted, deformed, text, watermark, signature",
}


def check_gpu():
    """Verifica se GPU est√° dispon√≠vel"""
    print("=" * 70)
    print("VERIFICANDO GPU")
    print("=" * 70)
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        print("\n‚úÖ GPU pronta para usar!")
        return True
    else:
        print("\n‚ö†Ô∏è  GPU n√£o encontrada. Usando CPU (ser√° muito mais lento)")
        response = input("Continuar mesmo assim? (s/n): ").strip().lower()
        return response in ['s', 'sim', 'y', 'yes']


def load_pipeline():
    """Carrega o pipeline do Stable Diffusion"""
    print("\n" + "=" * 70)
    print("CARREGANDO STABLE DIFFUSION")
    print("=" * 70)
    print(f"Modelo: {CONFIG['model']}")
    print("‚è≥ Primeira vez pode demorar (baixando modelo ~4GB)...")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    try:
        # Carregar pipeline
        pipe = StableDiffusionPipeline.from_pretrained(
            CONFIG['model'],
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            safety_checker=None,  # Remover safety checker para velocidade
        )
        
        # Usar scheduler otimizado
        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
        
        pipe = pipe.to(device)
        
        # Otimiza√ß√µes para RTX 3050 (8GB VRAM)
        if device == "cuda":
            print("\nüîß Aplicando otimiza√ß√µes para GPU...")
            
            # Ativar xformers se dispon√≠vel (muito mais r√°pido)
            try:
                pipe.enable_xformers_memory_efficient_attention()
                print("   ‚úÖ xformers ativado")
            except:
                print("   ‚ö†Ô∏è  xformers n√£o dispon√≠vel (instale com: pip install xformers)")
            
            # Ativar attention slicing (economiza VRAM)
            pipe.enable_attention_slicing(1)
            print("   ‚úÖ Attention slicing ativado")
            
            # Ativar VAE slicing (economiza mais VRAM)
            pipe.enable_vae_slicing()
            print("   ‚úÖ VAE slicing ativado")
        
        print("\n‚úÖ Pipeline carregado com sucesso!")
        return pipe
    
    except Exception as e:
        print(f"\n‚ùå Erro ao carregar pipeline: {e}")
        return None


def generate_image(pipe, prompt, negative_prompt=None, seed=None):
    """Gera uma imagem com Stable Diffusion"""
    
    if seed is not None:
        generator = torch.Generator("cuda" if torch.cuda.is_available() else "cpu").manual_seed(seed)
    else:
        generator = None
    
    start_time = time.time()
    
    with torch.inference_mode():
        image = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt or CONFIG['negative_prompt'],
            num_inference_steps=CONFIG['num_inference_steps'],
            guidance_scale=CONFIG['guidance_scale'],
            width=CONFIG['width'],
            height=CONFIG['height'],
            generator=generator,
        ).images[0]
    
    elapsed = time.time() - start_time
    
    return image, elapsed


def generate_from_structure(structure_path, output_folder):
    """Gera imagens baseado na estrutura JSON"""
    
    # Carregar estrutura
    with open(structure_path, 'r', encoding='utf-8') as f:
        structure = json.load(f)
    
    print("\n" + "=" * 70)
    print(f"GERANDO IMAGENS PARA: {structure['title']}")
    print("=" * 70)
    print(f"Total de p√°ginas: {len(structure['pages'])}")
    print(f"Pasta de sa√≠da: {output_folder}")
    
    # Criar pasta se n√£o existe
    os.makedirs(output_folder, exist_ok=True)
    
    # Carregar pipeline
    pipe = load_pipeline()
    if pipe is None:
        return False
    
    print("\n" + "=" * 70)
    print("GERANDO IMAGENS")
    print("=" * 70)
    
    total_time = 0
    
    for i, page in enumerate(structure['pages'], 1):
        print(f"\nüìÑ P√°gina {i}/{len(structure['pages'])}")
        print(f"   Tipo: {page['type']}")
        
        # Melhorar prompt para estilo infantil com tons past√©is
        enhanced_prompt = f"Children's book illustration, soft pastel colors, watercolor style, gentle and calm, {page['illustration_description']}"
        
        print(f"   Prompt: {enhanced_prompt[:80]}...")
        
        # Gerar imagem
        print(f"   üé® Gerando... ", end='', flush=True)
        image, elapsed = generate_image(pipe, enhanced_prompt, seed=42+i)
        total_time += elapsed
        
        print(f"[{elapsed:.1f}s]")
        
        # Salvar
        filename = f"page_{i:03d}_sd.png"
        filepath = os.path.join(output_folder, filename)
        image.save(filepath)
        
        print(f"   ‚úÖ Salva: {filename}")
    
    avg_time = total_time / len(structure['pages'])
    
    print("\n" + "=" * 70)
    print("‚ú® GERA√á√ÉO CONCLU√çDA!")
    print("=" * 70)
    print(f"‚úÖ {len(structure['pages'])} imagens geradas")
    print(f"‚è±Ô∏è  Tempo total: {total_time:.1f}s")
    print(f"‚è±Ô∏è  Tempo m√©dio por imagem: {avg_time:.1f}s")
    print(f"üìÅ Localiza√ß√£o: {output_folder}/")
    
    return True


def main():
    """Fun√ß√£o principal"""
    
    # Verificar GPU
    if not check_gpu():
        print("\n‚ùå Opera√ß√£o cancelada.")
        return 1
    
    # Verificar se tem estrutura do Genesis
    default_structure = "output/as_maravilhosas_historias_de_genesis/structure.json"
    
    if os.path.exists(default_structure):
        print(f"\nüìö Estrutura encontrada: {default_structure}")
        response = input("Deseja gerar imagens para este e-book? (s/n): ").strip().lower()
        
        if response in ['s', 'sim', 'y', 'yes']:
            output_folder = "output/as_maravilhosas_historias_de_genesis"
            success = generate_from_structure(default_structure, output_folder)
            return 0 if success else 1
    
    # Modo manual
    print("\nüìù Modo manual:")
    prompt = input("Digite a descri√ß√£o da imagem: ").strip()
    
    if not prompt:
        print("‚ùå Prompt vazio.")
        return 1
    
    pipe = load_pipeline()
    if pipe is None:
        return 1
    
    print("\nüé® Gerando imagem...")
    image, elapsed = generate_image(pipe, prompt)
    
    filename = "output_sd.png"
    image.save(filename)
    
    print(f"\n‚úÖ Imagem gerada em {elapsed:.1f}s")
    print(f"üìÅ Salva em: {filename}")
    
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Opera√ß√£o cancelada pelo usu√°rio.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
